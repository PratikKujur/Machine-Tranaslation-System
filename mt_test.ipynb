{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9965851,"sourceType":"datasetVersion","datasetId":6130561}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==2.12.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:04:32.421601Z","iopub.execute_input":"2024-11-21T16:04:32.421912Z","iopub.status.idle":"2024-11-21T16:05:43.200405Z","shell.execute_reply.started":"2024-11-21T16:04:32.421883Z","shell.execute_reply":"2024-11-21T16:05:43.199603Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.12.0\n  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m727.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (0.2.0)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (24.3.25)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (18.1.1)\nCollecting tensorflow-estimator<2.13,>=2.12.0\n  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.66.2)\nCollecting gast<=0.4.0,>=0.2.1\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.16.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (3.4.0)\nCollecting numpy<1.24,>=1.22\n  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting keras<2.13,>=2.12.0\n  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (3.20.3)\nCollecting tensorboard<2.13,>=2.12\n  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (0.37.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (24.1)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (3.12.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (2.1.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (2.4.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (65.5.1)\nRequirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (0.4.23)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (4.12.2)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.6.3)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.44.0)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.3.2)\nRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.14.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.35.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.4)\nCollecting google-auth-oauthlib<1.1,>=0.5\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.2.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.5)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, oauthlib, numpy, keras, gast, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.16.0\n    Uninstalling wrapt-1.16.0:\n      Successfully uninstalled wrapt-1.16.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: keras\n    Found existing installation: keras 3.5.0\n    Uninstalling keras-3.5.0:\n      Successfully uninstalled keras-3.5.0\n  Attempting uninstall: gast\n    Found existing installation: gast 0.6.0\n    Uninstalling gast-0.6.0:\n      Successfully uninstalled gast-0.6.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.16.2\n    Uninstalling tensorboard-2.16.2:\n      Successfully uninstalled tensorboard-2.16.2\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.16.1\n    Uninstalling tensorflow-2.16.1:\n      Successfully uninstalled tensorflow-2.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.12.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.12.0 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\nalbumentations 1.4.17 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nalbucore 0.0.17 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.12.0 numpy-1.23.5 oauthlib-3.2.2 requests-oauthlib-2.0.0 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:05:43.202419Z","iopub.execute_input":"2024-11-21T16:05:43.202734Z","iopub.status.idle":"2024-11-21T16:05:47.951235Z","shell.execute_reply.started":"2024-11-21T16:05:43.202702Z","shell.execute_reply":"2024-11-21T16:05:47.950377Z"}},"outputs":[{"name":"stdout","text":"Collecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk) (4.66.5)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk) (2024.9.11)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk) (1.4.2)\nInstalling collected packages: nltk\nSuccessfully installed nltk-3.9.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.translate.bleu_score import corpus_bleu\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:05:47.952608Z","iopub.execute_input":"2024-11-21T16:05:47.952913Z","iopub.status.idle":"2024-11-21T16:06:02.881589Z","shell.execute_reply.started":"2024-11-21T16:05:47.952881Z","shell.execute_reply":"2024-11-21T16:06:02.880605Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"np.random.seed(42)\ntf.random.set_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:02.882889Z","iopub.execute_input":"2024-11-21T16:06:02.883577Z","iopub.status.idle":"2024-11-21T16:06:02.887678Z","shell.execute_reply.started":"2024-11-21T16:06:02.883529Z","shell.execute_reply":"2024-11-21T16:06:02.886931Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df=pd.read_parquet(\"/kaggle/input/engtohin/train-00000-of-00001-71c2cec7402cd444.parquet\",engine=\"pyarrow\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:02.889847Z","iopub.execute_input":"2024-11-21T16:06:02.890132Z","iopub.status.idle":"2024-11-21T16:06:03.651798Z","shell.execute_reply.started":"2024-11-21T16:06:02.890106Z","shell.execute_reply":"2024-11-21T16:06:03.650946Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                    english_sentence  \\\n0  However, Paes, who was partnering Australia's ...   \n1  Whosoever desires the reward of the world, wit...   \n2  The value of insects in the biosphere is enorm...   \n3  Mithali To Anchor Indian Team Against Australi...   \n4  After the assent of the Honble President on 8t...   \n\n                                      hindi_sentence  \n0  आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...  \n1  और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...  \n2  जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...  \n3    आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को  \n4  8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>However, Paes, who was partnering Australia's ...</td>\n      <td>आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Whosoever desires the reward of the world, wit...</td>\n      <td>और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The value of insects in the biosphere is enorm...</td>\n      <td>जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mithali To Anchor Indian Team Against Australi...</td>\n      <td>आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>After the assent of the Honble President on 8t...</td>\n      <td>8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:03.652988Z","iopub.execute_input":"2024-11-21T16:06:03.653605Z","iopub.status.idle":"2024-11-21T16:06:03.688845Z","shell.execute_reply.started":"2024-11-21T16:06:03.653567Z","shell.execute_reply":"2024-11-21T16:06:03.687989Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def tokenize_with_special_tokens(data, lang_key, num_words=None):\n    tokenizer = Tokenizer(num_words=num_words, filters='', lower=True, oov_token=\"<unk>\")\n    special_tokens = [\"<sos>\", \"<eos>\"]\n    tokenizer.fit_on_texts(special_tokens + data[lang_key].tolist())\n    return tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:03.689923Z","iopub.execute_input":"2024-11-21T16:06:03.690493Z","iopub.status.idle":"2024-11-21T16:06:03.694549Z","shell.execute_reply.started":"2024-11-21T16:06:03.690464Z","shell.execute_reply":"2024-11-21T16:06:03.693786Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"en_tokenizer = tokenize_with_special_tokens(train_data, \"english_sentence\")\nhi_tokenizer = tokenize_with_special_tokens(train_data, \"hindi_sentence\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:03.695494Z","iopub.execute_input":"2024-11-21T16:06:03.695750Z","iopub.status.idle":"2024-11-21T16:06:07.275945Z","shell.execute_reply.started":"2024-11-21T16:06:03.695726Z","shell.execute_reply":"2024-11-21T16:06:07.274965Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def preprocess_with_special_tokens(data, en_tokenizer, hi_tokenizer, max_seq_len):\n    en_sequences = en_tokenizer.texts_to_sequences(data['english_sentence'])\n    hi_sequences = [\"<sos> \" + sent + \" <eos>\" for sent in data['hindi_sentence']]\n    hi_sequences = hi_tokenizer.texts_to_sequences(hi_sequences)\n\n    en_sequences = pad_sequences(en_sequences, maxlen=max_seq_len, padding=\"post\")\n    hi_sequences = pad_sequences(hi_sequences, maxlen=max_seq_len + 2, padding=\"post\")\n\n    decoder_input = hi_sequences[:, :-1]\n    decoder_target = hi_sequences[:, 1:]\n\n    return en_sequences, decoder_input, decoder_target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:07.277205Z","iopub.execute_input":"2024-11-21T16:06:07.277614Z","iopub.status.idle":"2024-11-21T16:06:07.282762Z","shell.execute_reply.started":"2024-11-21T16:06:07.277584Z","shell.execute_reply":"2024-11-21T16:06:07.281995Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_seq_len = 20\nen_train, dec_train_input, dec_train_target = preprocess_with_special_tokens(train_data, en_tokenizer, hi_tokenizer, max_seq_len)\nen_val, dec_val_input, dec_val_target = preprocess_with_special_tokens(val_data, en_tokenizer, hi_tokenizer, max_seq_len)\nen_test, _, _ = preprocess_with_special_tokens(test_data, en_tokenizer, hi_tokenizer, max_seq_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:07.283858Z","iopub.execute_input":"2024-11-21T16:06:07.284220Z","iopub.status.idle":"2024-11-21T16:06:12.047492Z","shell.execute_reply.started":"2024-11-21T16:06:07.284191Z","shell.execute_reply":"2024-11-21T16:06:12.046295Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"embedding_dim = 128\nhidden_units = 256\nvocab_size_en = len(en_tokenizer.word_index) + 1\nvocab_size_hi = len(hi_tokenizer.word_index) + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:12.048653Z","iopub.execute_input":"2024-11-21T16:06:12.048941Z","iopub.status.idle":"2024-11-21T16:06:12.053168Z","shell.execute_reply.started":"2024-11-21T16:06:12.048913Z","shell.execute_reply":"2024-11-21T16:06:12.052339Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"encoder_inputs = Input(shape=(None,))\nencoder_embedding = Embedding(input_dim=vocab_size_en, output_dim=embedding_dim)(encoder_inputs)\nencoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\nencoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:12.054330Z","iopub.execute_input":"2024-11-21T16:06:12.054754Z","iopub.status.idle":"2024-11-21T16:06:12.403715Z","shell.execute_reply.started":"2024-11-21T16:06:12.054722Z","shell.execute_reply":"2024-11-21T16:06:12.402680Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"decoder_inputs = Input(shape=(None,))\ndecoder_embedding = Embedding(input_dim=vocab_size_hi, output_dim=embedding_dim)(decoder_inputs)\ndecoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[encoder_state_h, encoder_state_c])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:12.404965Z","iopub.execute_input":"2024-11-21T16:06:12.405264Z","iopub.status.idle":"2024-11-21T16:06:12.667997Z","shell.execute_reply.started":"2024-11-21T16:06:12.405234Z","shell.execute_reply":"2024-11-21T16:06:12.667075Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"attention_layer = Attention()\ncontext_vector = attention_layer([decoder_outputs, encoder_outputs])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:12.670792Z","iopub.execute_input":"2024-11-21T16:06:12.671057Z","iopub.status.idle":"2024-11-21T16:06:12.681658Z","shell.execute_reply.started":"2024-11-21T16:06:12.671030Z","shell.execute_reply":"2024-11-21T16:06:12.680788Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\ndecoder_dense = Dense(vocab_size_hi, activation=\"softmax\")\ndecoder_outputs = decoder_dense(decoder_combined_context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:12.682713Z","iopub.execute_input":"2024-11-21T16:06:12.682970Z","iopub.status.idle":"2024-11-21T16:06:12.795573Z","shell.execute_reply.started":"2024-11-21T16:06:12.682945Z","shell.execute_reply":"2024-11-21T16:06:12.794505Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:12.796892Z","iopub.execute_input":"2024-11-21T16:06:12.797411Z","iopub.status.idle":"2024-11-21T16:06:12.847001Z","shell.execute_reply.started":"2024-11-21T16:06:12.797349Z","shell.execute_reply":"2024-11-21T16:06:12.846172Z"}},"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n embedding (Embedding)          (None, None, 128)    10919424    ['input_1[0][0]']                \n                                                                                                  \n embedding_1 (Embedding)        (None, None, 128)    10239744    ['input_2[0][0]']                \n                                                                                                  \n lstm (LSTM)                    [(None, None, 256),  394240      ['embedding[0][0]']              \n                                 (None, 256),                                                     \n                                 (None, 256)]                                                     \n                                                                                                  \n lstm_1 (LSTM)                  [(None, None, 256),  394240      ['embedding_1[0][0]',            \n                                 (None, 256),                     'lstm[0][1]',                   \n                                 (None, 256)]                     'lstm[0][2]']                   \n                                                                                                  \n attention (Attention)          (None, None, 256)    0           ['lstm_1[0][0]',                 \n                                                                  'lstm[0][0]']                   \n                                                                                                  \n concatenate (Concatenate)      (None, None, 512)    0           ['attention[0][0]',              \n                                                                  'lstm_1[0][0]']                 \n                                                                                                  \n dense (Dense)                  (None, None, 79998)  41038974    ['concatenate[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 62,986,622\nTrainable params: 62,986,622\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"batch_size = 64\nepochs = 30\nhistory = model.fit(\n    [en_train, dec_train_input], dec_train_target,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=([en_val, dec_val_input], dec_val_target)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:06:12.848121Z","iopub.execute_input":"2024-11-21T16:06:12.848403Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n1437/1437 [==============================] - 1323s 918ms/step - loss: 4.4495 - accuracy: 0.4231 - val_loss: 3.9300 - val_accuracy: 0.4588\nEpoch 2/30\n   4/1437 [..............................] - ETA: 20:43 - loss: 3.8572 - accuracy: 0.4541","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/machineTranslation_1.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_translation(input_text, model, en_tokenizer, hi_tokenizer, max_seq_len):\n    input_seq = en_tokenizer.texts_to_sequences([input_text])\n    input_seq = pad_sequences(input_seq, maxlen=max_seq_len, padding=\"post\")\n\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = hi_tokenizer.word_index['<sos>']\n\n    decoded_sentence = []\n\n    for _ in range(max_seq_len + 2):\n        predictions = model.predict([input_seq, target_seq], verbose=0)\n        predicted_token = np.argmax(predictions[0, -1, :])\n\n        sampled_word = hi_tokenizer.index_word.get(predicted_token, '<unk>')\n        if sampled_word == '<eos>':\n            break\n\n        decoded_sentence.append(sampled_word)\n\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = predicted_token\n\n    return ' '.join(decoded_sentence)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = \"who are you\"\npredicted_translation = predict_translation(input_text, model, en_tokenizer, hi_tokenizer, max_seq_len)\nprint(f\"Input: {input_text}\")\nprint(f\"Predicted Translation: {predicted_translation}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_bleu_score(model, data, en_tokenizer, hi_tokenizer, max_seq_len):\n    references = []\n    hypotheses = []\n\n    for _, row in data.iterrows():\n        input_text = row['english_sentence']\n        reference = row['hindi_sentence'].split()\n\n        predicted_translation = predict_translation(input_text, model, en_tokenizer, hi_tokenizer, max_seq_len)\n        hypothesis = predicted_translation.split()\n\n        references.append([reference])\n        hypotheses.append(hypothesis)\n\n    bleu_score = corpus_bleu(references, hypotheses)\n    return bleu_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nbleu_score = evaluate_bleu_score(model, test_data, en_tokenizer, hi_tokenizer, max_seq_len)\nprint(f\"Test BLEU Score: {bleu_score}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}